# Telegraf Configuration
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Collection offset is used to shift the collection by the given amount.
  ## This can be be used to avoid many plugins querying constraint devices
  ## at the same time by manually scheduling them in time.
  # collection_offset = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which
  ## translates by calling external programs snmptranslate and snmptable,
  ## or "gosmi" which translates using the built-in gosmi library.
  # snmp_translator = "netsnmp"

  ## Name of the file to load the state of plugins from and store the state to.
  ## If uncommented and not empty, this file will be used to save the state of
  ## stateful plugins on termination of Telegraf. If the file exists on start,
  ## the state in the file will be restored for the plugins.
  # statefile = ""


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# # Configuration for sending metrics to InfluxDB 2.0
# [[outputs.influxdb_v2]]
#   ## The URLs of the InfluxDB cluster nodes.
#   ##
#   ## Multiple URLs can be specified for a single cluster, only ONE of the
#   ## urls will be written to each interval.
#   ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
#   urls = ["http://127.0.0.1:8086"]
#
#   ## Token for authentication.
#   token = ""
#
#   ## Organization is the name of the organization you wish to write to.
#   organization = ""
#
#   ## Destination bucket to write into.
#   bucket = ""
#
#   ## The value of this tag will be used to determine the bucket.  If this
#   ## tag is not set the 'bucket' option is used as the default.
#   # bucket_tag = ""
#
#   ## If true, the bucket tag will not be added to the metric.
#   # exclude_bucket_tag = false
#
#   ## Timeout for HTTP messages.
#   # timeout = "5s"
#
#   ## Additional HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## HTTP Proxy override, if unset values the standard proxy environment
#   ## variables are consulted to determine which proxy, if any, should be used.
#   # http_proxy = "http://corporate.proxy:3128"
#
#   ## HTTP User-Agent
#   # user_agent = "telegraf"
#
#   ## Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "gzip"
#
#   ## Enable or disable uint support for writing uints influxdb 2.0.
#   # influx_uint_support = false
#
#   ## HTTP/2 Timeouts
#   ## The following values control the HTTP/2 client's timeouts. These settings
#   ## are generally not required unless a user is seeing issues with client
#   ## disconnects. If a user does see issues, then it is suggested to set these
#   ## values to "15s" for ping timeout and "30s" for read idle timeout and
#   ## retry.
#   ##
#   ## Note that the timer for read_idle_timeout begins at the end of the last
#   ## successful write and not at the beginning of the next write.
#   # ping_timeout = "0s"
#   # read_idle_timeout = "0s"
#
#   ## Optional TLS Config for use on HTTP connections.
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false

# # Configuration for Elasticsearch to send metrics to.
# [[outputs.elasticsearch]]
#   ## The full HTTP endpoint URL for your Elasticsearch instance
#   ## Multiple urls can be specified as part of the same cluster,
#   ## this means that only ONE of the urls will be written to each interval
#   urls = [ "http://node1.es.example.com:9200" ] # required.
#   ## Elasticsearch client timeout, defaults to "5s" if not set.
#   timeout = "5s"
#   ## Set to true to ask Elasticsearch a list of all cluster nodes,
#   ## thus it is not necessary to list all nodes in the urls config option
#   enable_sniffer = false
#   ## Set to true to enable gzip compression
#   enable_gzip = false
#   ## Set the interval to check if the Elasticsearch nodes are available
#   ## Setting to "0s" will disable the health check (not recommended in production)
#   health_check_interval = "10s"
#   ## Set the timeout for periodic health checks.
#   # health_check_timeout = "1s"
#   ## HTTP basic authentication details.
#   ## HTTP basic authentication details
#   # username = "telegraf"
#   # password = "mypassword"
#   ## HTTP bearer token authentication details
#   # auth_bearer_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9"
#
#   ## Index Config
#   ## The target index for metrics (Elasticsearch will create if it not exists).
#   ## You can use the date specifiers below to create indexes per time frame.
#   ## The metric timestamp will be used to decide the destination index name
#   # %Y - year (2016)
#   # %y - last two digits of year (00..99)
#   # %m - month (01..12)
#   # %d - day of month (e.g., 01)
#   # %H - hour (00..23)
#   # %V - week of the year (ISO week) (01..53)
#   ## Additionally, you can specify a tag name using the notation {{tag_name}}
#   ## which will be used as part of the index name. If the tag does not exist,
#   ## the default tag value will be used.
#   # index_name = "telegraf-{{host}}-%Y.%m.%d"
#   # default_tag_value = "none"
#   index_name = "telegraf-%Y.%m.%d" # required.
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Template Config
#   ## Set to true if you want telegraf to manage its index template.
#   ## If enabled it will create a recommended index template for telegraf indexes
#   manage_template = true
#   ## The template name used for telegraf indexes
#   template_name = "telegraf"
#   ## Set to true if you want telegraf to overwrite an existing template
#   overwrite_template = false
#   ## If set to true a unique ID hash will be sent as sha256(concat(timestamp,measurement,series-hash)) string
#   ## it will enable data resend and update metric points avoiding duplicated metrics with diferent id's
#   force_document_id = false
#
#   ## Specifies the handling of NaN and Inf values.
#   ## This option can have the following values:
#   ##    none    -- do not modify field-values (default); will produce an error if NaNs or infs are encountered
#   ##    drop    -- drop fields containing NaNs or infs
#   ##    replace -- replace with the value in "float_replacement_value" (default: 0.0)
#   ##               NaNs and inf will be replaced with the given number, -inf with the negative of that number
#   # float_handling = "none"
#   # float_replacement_value = 0.0
#
#   ## Pipeline Config
#   ## To use a ingest pipeline, set this to the name of the pipeline you want to use.
#   # use_pipeline = "my_pipeline"
#   ## Additionally, you can specify a tag name using the notation {{tag_name}}
#   ## which will be used as part of the pipeline name. If the tag does not exist,
#   ## the default pipeline will be used as the pipeline. If no default pipeline is set,
#   ## no pipeline is used for the metric.
#   # use_pipeline = "{{es_pipeline}}"
#   # default_pipeline = "my_pipeline"


# # A plugin that can transmit metrics over HTTP
# [[outputs.http]]
#   ## URL is the address to send metrics to
#   url = "http://127.0.0.1:8080/telegraf"
#
#   ## Timeout for HTTP message
#   # timeout = "5s"
#
#   ## HTTP method, one of: "POST" or "PUT" or "PATCH"
#   # method = "POST"
#
#   ## HTTP Basic Auth credentials
#   # username = "username"
#   # password = "pa$$word"
#
#   ## OAuth2 Client Credentials Grant
#   # client_id = "clientid"
#   # client_secret = "secret"
#   # token_url = "https://indentityprovider/oauth2/v1/token"
#   # audience = ""
#   # scopes = ["urn:opc:idm:__myscopes__"]
#
#   ## Goole API Auth
#   # google_application_credentials = "/etc/telegraf/example_secret.json"
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional Cookie authentication
#   # cookie_auth_url = "https://localhost/authMe"
#   # cookie_auth_method = "POST"
#   # cookie_auth_username = "username"
#   # cookie_auth_password = "pa$$word"
#   # cookie_auth_headers = '{"Content-Type": "application/json", "X-MY-HEADER":"hello"}'
#   # cookie_auth_body = '{"username": "user", "password": "pa$$word", "authenticate": "me"}'
#   ## cookie_auth_renewal not set or set to "0" will auth once and never renew the cookie
#   # cookie_auth_renewal = "5m"
#
#   ## Data format to output.
#   ## Each data format has it's own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
#
#   ## Use batch serialization format (default) instead of line based format.
#   ## Batch format is more efficient and should be used unless line based
#   ## format is really needed.
#   # use_batch_format = true
#
#   ## HTTP Content-Encoding for write request body, can be set to "gzip" to
#   ## compress body or "identity" to apply no encoding.
#   # content_encoding = "identity"
#
#   ## MaxIdleConns controls the maximum number of idle (keep-alive)
#   ## connections across all hosts. Zero means no limit.
#   # max_idle_conn = 0
#
#   ## MaxIdleConnsPerHost, if non-zero, controls the maximum idle
#   ## (keep-alive) connections to keep per-host. If zero,
#   ## DefaultMaxIdleConnsPerHost is used(2).
#   # max_idle_conn_per_host = 2
#
#   ## Idle (keep-alive) connection timeout.
#   ## Maximum amount of time before idle connection is closed.
#   ## Zero means no limit.
#   # idle_conn_timeout = 0
#
#   ## Amazon Region
#   #region = "us-east-1"
#
#   ## Amazon Credentials
#   ## Amazon Credentials are not built unless the following aws_service
#   ## setting is set to a non-empty string. It may need to match the name of
#   ## the service output to as well
#   #aws_service = "execute-api"
#
#   ## Credentials are loaded in the following order
#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified
#   ## 2) Assumed credentials via STS if role_arn is specified
#   ## 3) explicit credentials from 'access_key' and 'secret_key'
#   ## 4) shared profile from 'profile'
#   ## 5) environment variables
#   ## 6) shared credentials file
#   ## 7) EC2 Instance Profile
#   #access_key = ""
#   #secret_key = ""
#   #token = ""
#   #role_arn = ""
#   #web_identity_token_file = ""
#   #role_session_name = ""
#   #profile = ""
#   #shared_credential_file = ""
#
#   ## Optional list of statuscodes (<200 or >300) upon which requests should not be retried
#   # non_retryable_statuscodes = [409, 413]
#
#   ## NOTE: Due to the way TOML is parsed, tables must be at the END of the
#   ## plugin definition, otherwise additional config options are read as part of
#   ## the table
#
#   ## Additional HTTP headers
#   # [outputs.http.headers]
#   #   ## Should be set manually to "application/json" for json data_format
#   #   Content-Type = "text/plain; charset=utf-8"





# # Configuration for MQTT server to send metrics to
# [[outputs.mqtt]]
#   ## MQTT Brokers
#   ## The list of brokers should only include the hostname or IP address and the
#   ## port to the broker. This should follow the format `[{scheme}://]{host}:{port}`. For
#   ## example, `localhost:1883` or `mqtt://localhost:1883`.
#   ## Scheme can be any of the following: tcp://, mqtt://, tls://, mqtts://
#   ## non-TLS and TLS servers can not be mix-and-matched.
#   servers = ["localhost:1883", ] # or ["mqtts://tls.example.com:1883"]
#
#   ## Protocol can be `3.1.1` or `5`. Default is `3.1.1`
#   # protocol = "3.1.1"
#
#   ## MQTT Topic for Producer Messages
#   ## MQTT outputs send metrics to this topic format:
#   ## {{ .TopicPrefix }}/{{ .Hostname }}/{{ .PluginName }}/{{ .Tag "tag_key" }}
#   ## (e.g. prefix/web01.example.com/mem/some_tag_value)
#   ## Each path segment accepts either a template placeholder, an environment variable, or a tag key
#   ## of the form `{{.Tag "tag_key_name"}}`. Empty path elements as well as special MQTT characters
#   ## (such as `+` or `#`) are invalid to form the topic name and will lead to an error.
#   ## In case a tag is missing in the metric, that path segment omitted for the final topic.
#   topic = "telegraf/{{ .Hostname }}/{{ .PluginName }}"
#
#   ## QoS policy for messages
#   ## The mqtt QoS policy for sending messages.
#   ## See https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q029090_.htm
#   ##   0 = at most once
#   ##   1 = at least once
#   ##   2 = exactly once
#   # qos = 2
#
#   ## Keep Alive
#   ## Defines the maximum length of time that the broker and client may not
#   ## communicate. Defaults to 0 which turns the feature off.
#   ##
#   ## For version v2.0.12 and later mosquitto there is a bug
#   ## (see https://github.com/eclipse/mosquitto/issues/2117), which requires
#   ## this to be non-zero. As a reference eclipse/paho.mqtt.golang defaults to 30.
#   # keep_alive = 0
#
#   ## username and password to connect MQTT server.
#   # username = "telegraf"
#   # password = "metricsmetricsmetricsmetrics"
#
#   ## client ID
#   ## The unique client id to connect MQTT server. If this parameter is not set
#   ## then a random ID is generated.
#   # client_id = ""
#
#   ## Timeout for write operations. default: 5s
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## When true, metrics will be sent in one MQTT message per flush. Otherwise,
#   ## metrics are written one metric per MQTT message.
#   ## DEPRECATED: Use layout option instead
#   # batch = false
#
#   ## When true, metric will have RETAIN flag set, making broker cache entries until someone
#   ## actually reads it
#   # retain = false
#
#   ## Layout of the topics published.
#   ## The following choices are available:
#   ##   non-batch -- send individual messages, one for each metric
#   ##   batch     -- send all metric as a single message per MQTT topic
#   ## NOTE: The following options will ignore the 'data_format' option and send single values
#   ##   field     -- send individual messages for each field, appending its name to the metric topic
#   ##   homie-v4  -- send metrics with fields and tags according to the 4.0.0 specs
#   ##                see https://homieiot.github.io/specification/
#   # layout = "non-batch"
#
#   ## HOMIE specific settings
#   ## The following options provide templates for setting the device name
#   ## and the node-ID for the topics. Both options are MANDATORY and can contain
#   ## {{ .PluginName }} (metric name), {{ .Tag "key"}} (tag reference to 'key')
#   ## or constant strings. The templays MAY NOT contain slashes!
#   # homie_device_name = ""
#   # homie_node_id = ""
#
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
#
#   ## NOTE: Due to the way TOML is parsed, tables must be at the END of the
#   ## plugin definition, otherwise additional config options are read as part of
#   ## the table
#
#   ## Optional MQTT 5 publish properties
#   ## These setting only apply if the "protocol" property is set to 5. This must
#   ## be defined at the end of the plugin settings, otherwise TOML will assume
#   ## anything else is part of this table. For more details on publish properties
#   ## see the spec:
#   ## https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html#_Toc3901109
#   # [outputs.mqtt.v5]
#   #   content_type = ""
#   #   response_topic = ""
#   #   message_expiry = "0s"
#   #   topic_alias = 0
#   # [outputs.mqtt.v5.user_properties]
#   #   "key1" = "value 1"
#   #   "key2" = "value 2"




# # Publishes metrics to a redis timeseries server
# [[outputs.redistimeseries]]
#   ## The address of the  server.
#   address = "127.0.0.1:6379"
#
#   ## Redis ACL credentials
#   # username = ""
#   # password = ""
#   # database = 0
#
#   ## Timeout for operations such as ping or sending metrics
#   # timeout = "10s"
#
#   ## Enable attempt to convert string fields to numeric values
#   ## If "false" or in case the string value cannot be converted the string
#   ## field will be dropped.
#   # convert_string_fields = true
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   # insecure_skip_verify = false


# # A plugin that can transmit metrics over WebSocket.
# [[outputs.websocket]]
#   ## URL is the address to send metrics to. Make sure ws or wss scheme is used.
#   url = "ws://127.0.0.1:3000/telegraf"
#
#   ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).
#   # connect_timeout = "30s"
#   # write_timeout = "30s"
#   # read_timeout = "30s"
#
#   ## Optionally turn on using text data frames (binary by default).
#   # use_text_frames = false
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional SOCKS5 proxy to use
#   # socks5_enabled = true
#   # socks5_address = "127.0.0.1:1080"
#   # socks5_username = "alice"
#   # socks5_password = "pass123"
#
#   ## Optional HTTP proxy to use
#   # use_system_proxy = false
#   # http_proxy_url = "http://localhost:8888"
#
#   ## Data format to output.
#   ## Each data format has it's own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   # data_format = "influx"
#
#   ## NOTE: Due to the way TOML is parsed, tables must be at the END of the
#   ## plugin definition, otherwise additional config options are read as part of
#   ## the table
#
#   ## Additional HTTP Upgrade headers
#   # [outputs.websocket.headers]
#   #   Authorization = "Bearer <TOKEN>"


# # Send aggregated metrics to Yandex.Cloud Monitoring
# [[outputs.yandex_cloud_monitoring]]
#   ## Timeout for HTTP writes.
#   # timeout = "20s"
#
#   ## Yandex.Cloud monitoring API endpoint. Normally should not be changed
#   # endpoint_url = "https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write"
#
#   ## All user metrics should be sent with "custom" service specified. Normally should not be changed
#   # service = "custom"


###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################

# # Dates measurements, tags, and fields that pass through this filter.
# [[processors.date]]
#   ## New tag to create
#   tag_key = "month"
#
#   ## New field to create (cannot set both field_key and tag_key)
#   # field_key = "month"
#
#   ## Date format string, must be a representation of the Go "reference time"
#   ## which is "Mon Jan 2 15:04:05 -0700 MST 2006".
#   date_format = "Jan"
#
#   ## If destination is a field, date format can also be one of
#   ## "unix", "unix_ms", "unix_us", or "unix_ns", which will insert an integer field.
#   # date_format = "unix"
#
#   ## Offset duration added to the date string when writing the new tag.
#   # date_offset = "0s"
#
#   ## Timezone to use when creating the tag or field using a reference time
#   ## string.  This can be set to one of "UTC", "Local", or to a location name
#   ## in the IANA Time Zone database.
#   ##   example: timezone = "America/Los_Angeles"
#   # timezone = "UTC"




# ## Set default fields on your metric(s) when they are nil or empty
# [[processors.defaults]]
#   ## Ensures a set of fields always exists on your metric(s) with their
#   ## respective default value.
#   ## For any given field pair (key = default), if it's not set, a field
#   ## is set on the metric with the specified default.
#   ##
#   ## A field is considered not set if it is nil on the incoming metric;
#   ## or it is not nil but its value is an empty string or is a string
#   ## of one or more spaces.
#   ##   <target-field> = <value>
#   [processors.defaults.fields]
#     field_1 = "bar"
#     time_idle = 0
#     is_error = true


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  ## NOTE: The resulting 'time_active' field INCLUDES 'iowait'!
  report_active = false
  ## If true and the info is available then add core_id and physical_id tags
  core_tags = false


# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

  ## Ignore mount points by mount options.
  ## The 'mount' command reports options of all mounts in parathesis.
  ## Bind mounts can be ignored with the special 'bind' option.
  # ignore_mount_opts = []


# Read metrics about disk IO by device
[[inputs.diskio]]
  ## By default, telegraf will gather stats for all devices including
  ## disk partitions.
  ## Setting devices will restrict the stats to the specified devices.
  ## NOTE: Globbing expressions (e.g. asterix) are not supported for
  ##       disk synonyms like '/dev/disk/by-id'.
  # devices = ["sda", "sdb", "vd*", "/dev/disk/by-id/nvme-eui.00123deadc0de123"]
  ## Uncomment the following line if you need disk serial numbers.
  # skip_serial_number = false
  #
  ## On systems which support it, device metadata can be added in the form of
  ## tags.
  ## Currently only Linux is supported via udev properties. You can view
  ## available properties for a device by running:
  ## 'udevadm info -q property -n /dev/sda'
  ## Note: Most, but not all, udev properties can be accessed this way. Properties
  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  # device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]
  #
  ## Using the same metadata source as device_tags, you can also customize the
  ## name of the device via templates.
  ## The 'name_templates' parameter is a list of templates to try and apply to
  ## the device. The template may contain variables in the form of '$PROPERTY' or
  ## '${PROPERTY}'. The first template which does not contain any variables not
  ## present for the device is used as the device name tag.
  ## The typical use case is for LVM volumes, to get the VG/LV name instead of
  ## the near-meaningless DM-0 name.
  # name_templates = ["$ID_FS_LABEL","$DM_VG_NAME/$DM_LV_NAME"]


# Plugin to collect various Linux kernel statistics.
# This plugin ONLY supports Linux
[[inputs.kernel]]
  ## Additional gather options
  ## Possible options include:
  ## * ksm - kernel same-page merging
  # collect = []


# Read metrics about memory usage
[[inputs.mem]]
  # no configuration


# Get the number of processes and group them by status
# This plugin ONLY supports non-Windows
[[inputs.processes]]
  ## Use sudo to run ps command on *BSD systems. Linux systems will read
  ## /proc, so this does not apply there.
  # use_sudo = false


# Read metrics about swap memory usage
# This plugin ONLY supports Linux
[[inputs.swap]]
  # no configuration


# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration


# # Gather ActiveMQ metrics
# [[inputs.activemq]]
#   ## ActiveMQ WebConsole URL
#   url = "http://127.0.0.1:8161"
#
#   ## Required ActiveMQ Endpoint
#   ##   deprecated in 1.11; use the url option
#   # server = "192.168.50.10"
#   # port = 8161
#
#   ## Credentials for basic HTTP authentication
#   # username = "admin"
#   # password = "admin"
#
#   ## Required ActiveMQ webadmin root path
#   # webadmin = "admin"
#
#   ## Maximum time to receive response.
#   # response_timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false

# # Read metrics about docker containers
# [[inputs.docker]]
#   ## Docker Endpoint
#   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
#   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
#   endpoint = "unix:///var/run/docker.sock"
#
#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)
#   ## Note: configure this in one of the manager nodes in a Swarm cluster.
#   ## configuring in multiple Swarm managers results in duplication of metrics.
#   gather_services = false
#
#   ## Only collect metrics for these containers. Values will be appended to
#   ## container_name_include.
#   ## Deprecated (1.4.0), use container_name_include
#   container_names = []
#
#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars
#   source_tag = false
#
#   ## Containers to include and exclude. Collect all if empty. Globs accepted.
#   container_name_include = []
#   container_name_exclude = []
#
#   ## Container states to include and exclude. Globs accepted.
#   ## When empty only containers in the "running" state will be captured.
#   ## example: container_state_include = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   ## example: container_state_exclude = ["created", "restarting", "running", "removing", "paused", "exited", "dead"]
#   # container_state_include = []
#   # container_state_exclude = []
#
#   ## Timeout for docker list, info, and stats commands
#   timeout = "5s"
#
#   ## Whether to report for each container per-device blkio (8:0, 8:1...),
#   ## network (eth0, eth1, ...) and cpu (cpu0, cpu1, ...) stats or not.
#   ## Usage of this setting is discouraged since it will be deprecated in favor of 'perdevice_include'.
#   ## Default value is 'true' for backwards compatibility, please set it to 'false' so that 'perdevice_include' setting
#   ## is honored.
#   perdevice = true
#
#   ## Specifies for which classes a per-device metric should be issued
#   ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)
#   ## Please note that this setting has no effect if 'perdevice' is set to 'true'
#   # perdevice_include = ["cpu"]
#
#   ## Whether to report for each container total blkio and network stats or not.
#   ## Usage of this setting is discouraged since it will be deprecated in favor of 'total_include'.
#   ## Default value is 'false' for backwards compatibility, please set it to 'true' so that 'total_include' setting
#   ## is honored.
#   total = false
#
#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.
#   ## Possible values are 'cpu', 'blkio' and 'network'
#   ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.
#   ## Please note that this setting has no effect if 'total' is set to 'false'
#   # total_include = ["cpu", "blkio", "network"]
#
#   ## docker labels to include and exclude as tags.  Globs accepted.
#   ## Note that an empty array for both will include all labels as tags
#   docker_label_include = []
#   docker_label_exclude = []
#
#   ## Which environment variables should we use as a tag
#   tag_env = ["JAVA_HOME", "HEAP_SIZE"]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# # Read stats from one or more Elasticsearch servers or clusters
# [[inputs.elasticsearch]]
#   ## specify a list of one or more Elasticsearch servers
#   ## you can add username and password to your url to use basic authentication:
#   ## servers = ["http://user:pass@localhost:9200"]
#   servers = ["http://localhost:9200"]
#
#   ## Timeout for HTTP requests to the elastic search server(s)
#   http_timeout = "5s"
#
#   ## When local is true (the default), the node will read only its own stats.
#   ## Set local to false when you want to read the node stats from all nodes
#   ## of the cluster.
#   local = true
#
#   ## Set cluster_health to true when you want to obtain cluster health stats
#   cluster_health = false
#
#   ## Adjust cluster_health_level when you want to obtain detailed health stats
#   ## The options are
#   ##  - indices (default)
#   ##  - cluster
#   # cluster_health_level = "indices"
#
#   ## Set cluster_stats to true when you want to obtain cluster stats.
#   cluster_stats = false
#
#   ## Only gather cluster_stats from the master node.
#   ## To work this require local = true
#   cluster_stats_only_from_master = true
#
#   ## Indices to collect; can be one or more indices names or _all
#   ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index
#   ## names that end with a changing value, like a date.
#   indices_include = ["_all"]
#
#   ## One of "shards", "cluster", "indices"
#   ## Currently only "shards" is implemented
#   indices_level = "shards"
#
#   ## node_stats is a list of sub-stats that you want to have gathered.
#   ## Valid options are "indices", "os", "process", "jvm", "thread_pool",
#   ## "fs", "transport", "http", "breaker". Per default, all stats are gathered.
#   # node_stats = ["jvm", "http"]
#
#   ## HTTP Basic Authentication username and password.
#   # username = ""
#   # password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Sets the number of most recent indices to return for indices that are
#   ## configured with a date-stamped suffix. Each 'indices_include' entry
#   ## ending with a wildcard (*) or glob matching pattern will group together
#   ## all indices that match it, and  sort them by the date or number after
#   ## the wildcard. Metrics then are gathered for only the
#   ## 'num_most_recent_indices' amount of most  recent indices.
#   # num_most_recent_indices = 0

# # Read metrics from one or more commands that can output to stdout
# [[inputs.exec]]
#   ## Commands array
#   commands = [
#     "/tmp/test.sh",
#     "/usr/bin/mycollector --foo=bar",
#     "/tmp/collect_*.sh"
#   ]
#
#   ## Environment variables
#   ## Array of "key=value" pairs to pass as environment variables
#   ## e.g. "KEY=value", "USERNAME=John Doe",
#   ## "LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs"
#   # environment = []
#
#   ## Timeout for each command to complete.
#   timeout = "5s"
#
#   ## measurement name suffix (for separating different commands)
#   name_suffix = "_mycollector"
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"


# # HTTP/HTTPS request given an address a method and a timeout
# [[inputs.http_response]]
#   ## List of urls to query.
#   # urls = ["http://localhost"]
#
#   ## Set http_proxy.
#   ## Telegraf uses the system wide proxy settings if it's is not set.
#   # http_proxy = "http://localhost:8888"
#
#   ## Set response_timeout (default 5 seconds)
#   # response_timeout = "5s"
#
#   ## HTTP Request Method
#   # method = "GET"
#
#   ## Whether to follow redirects from the server (defaults to false)
#   # follow_redirects = false
#
#   ## Optional file with Bearer token
#   ## file content is added as an Authorization header
#   # bearer_token = "/path/to/file"
#
#   ## Optional HTTP Basic Auth Credentials
#   # username = "username"
#   # password = "pa$$word"
#
#   ## Optional HTTP Request Body
#   # body = '''
#   # {'fake':'data'}
#   # '''
#
#   ## Optional name of the field that will contain the body of the response.
#   ## By default it is set to an empty String indicating that the body's
#   ## content won't be added
#   # response_body_field = ''
#
#   ## Maximum allowed HTTP response body size in bytes.
#   ## 0 means to use the default of 32MiB.
#   ## If the response body size exceeds this limit a "body_read_error" will
#   ## be raised.
#   # response_body_max_size = "32MiB"
#
#   ## Optional substring or regex match in body of the response (case sensitive)
#   # response_string_match = "\"service_status\": \"up\""
#   # response_string_match = "ok"
#   # response_string_match = "\".*_status\".?:.?\"up\""
#
#   ## Expected response status code.
#   ## The status code of the response is compared to this value. If they match,
#   ## the field "response_status_code_match" will be 1, otherwise it will be 0.
#   ## If the expected status code is 0, the check is disabled and the field
#   ## won't be added.
#   # response_status_code = 0
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = ""
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## HTTP Request Headers (all values must be strings)
#   # [inputs.http_response.headers]
#   #   Host = "github.com"
#
#   ## Optional setting to map response http headers into tags
#   ## If the http header is not present on the request, no corresponding tag will
#   ## be added. If multiple instances of the http header are present, only the
#   ## first value will be used.
#   # http_header_tags = {"HTTP_HEADER" = "TAG_NAME"}
#
#   ## Interface to use when dialing an address
#   # interface = "eth0"




# # Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints
# [[inputs.influxdb]]
#   ## Works with InfluxDB debug endpoints out of the box,
#   ## but other services can use this format too.
#   ## See the influxdb plugin's README for more details.
#
#   ## Multiple URLs from which to read InfluxDB-formatted JSON
#   ## Default is "http://localhost:8086/debug/vars".
#   urls = [
#     "http://localhost:8086/debug/vars"
#   ]
#
#   ## Username and password to send using HTTP Basic Authentication.
#   # username = ""
#   # password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## http request & header timeout
#   timeout = "5s"


# # Provides Linux CPU metrics
# # This plugin ONLY supports Linux
# [[inputs.linux_cpu]]
#   ## Path for sysfs filesystem.
#   ## See https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt
#   ## Defaults:
#   # host_sys = "/sys"
#
#   ## CPU metrics collected by the plugin.
#   ## Supported options:
#   ## "cpufreq", "thermal"
#   ## Defaults:
#   # metrics = ["cpufreq"]


# # Retrieve data from MODBUS slave devices
# [[inputs.modbus]]
#   ## Connection Configuration
#   ##
#   ## The plugin supports connections to PLCs via MODBUS/TCP, RTU over TCP, ASCII over TCP or
#   ## via serial line communication in binary (RTU) or readable (ASCII) encoding
#   ##
#   ## Device name
#   name = "Device"
#
#   ## Slave ID - addresses a MODBUS device on the bus
#   ## Range: 0 - 255 [0 = broadcast; 248 - 255 = reserved]
#   slave_id = 1
#
#   ## Timeout for each request
#   timeout = "1s"
#
#   ## Maximum number of retries and the time to wait between retries
#   ## when a slave-device is busy.
#   # busy_retries = 0
#   # busy_retries_wait = "100ms"
#
#   # TCP - connect via Modbus/TCP
#   controller = "tcp://localhost:502"
#
#   ## Serial (RS485; RS232)
#   ## For RS485 specific setting check the end of the configuration.
#   ## For unix-like operating systems use:
#   # controller = "file:///dev/ttyUSB0"
#   ## For Windows operating systems use:
#   # controller = "COM1"
#   # baud_rate = 9600
#   # data_bits = 8
#   # parity = "N"
#   # stop_bits = 1
#
#   ## Transmission mode for Modbus packets depending on the controller type.
#   ## For Modbus over TCP you can choose between "TCP" , "RTUoverTCP" and
#   ## "ASCIIoverTCP".
#   ## For Serial controllers you can choose between "RTU" and "ASCII".
#   ## By default this is set to "auto" selecting "TCP" for ModbusTCP connections
#   ## and "RTU" for serial connections.
#   # transmission_mode = "auto"
#
#   ## Trace the connection to the modbus device as debug messages
#   ## Note: You have to enable telegraf's debug mode to see those messages!
#   # debug_connection = false
#
#   ## Define the configuration schema
#   ##  |---register -- define fields per register type in the original style (only supports one slave ID)
#   ##  |---request  -- define fields on a requests base
#   ##  |---metric   -- define fields on a metric base
#   configuration_type = "register"
#   ## --- "register" configuration style ---
#
#   ## Measurements
#   ##
#
#   ## Digital Variables, Discrete Inputs and Coils
#   ## measurement - the (optional) measurement name, defaults to "modbus"
#   ## name        - the variable name
#   ## data_type   - the (optional) output type, can be BOOL or UINT16 (default)
#   ## address     - variable address
#
#   discrete_inputs = [
#     { name = "start",          address = [0]},
#     { name = "stop",           address = [1]},
#     { name = "reset",          address = [2]},
#     { name = "emergency_stop", address = [3]},
#   ]
#   coils = [
#     { name = "motor1_run",     address = [0]},
#     { name = "motor1_jog",     address = [1]},
#     { name = "motor1_stop",    address = [2]},
#   ]
#
#   ## Analog Variables, Input Registers and Holding Registers
#   ## measurement - the (optional) measurement name, defaults to "modbus"
#   ## name        - the variable name
#   ## byte_order  - the ordering of bytes
#   ##  |---AB, ABCD   - Big Endian
#   ##  |---BA, DCBA   - Little Endian
#   ##  |---BADC       - Mid-Big Endian
#   ##  |---CDAB       - Mid-Little Endian
#   ## data_type   - INT8L, INT8H, UINT8L, UINT8H (low and high byte variants)
#   ##               INT16, UINT16, INT32, UINT32, INT64, UINT64,
#   ##               FLOAT16-IEEE, FLOAT32-IEEE, FLOAT64-IEEE (IEEE 754 binary representation)
#   ##               FIXED, UFIXED (fixed-point representation on input)
#   ##               FLOAT32 is a deprecated alias for UFIXED for historic reasons, should be avoided
#   ## scale       - the final numeric variable representation
#   ## address     - variable address
#
#   holding_registers = [
#     { name = "power_factor", byte_order = "AB",   data_type = "FIXED", scale=0.01,  address = [8]},
#     { name = "voltage",      byte_order = "AB",   data_type = "FIXED", scale=0.1,   address = [0]},
#     { name = "energy",       byte_order = "ABCD", data_type = "FIXED", scale=0.001, address = [5,6]},
#     { name = "current",      byte_order = "ABCD", data_type = "FIXED", scale=0.001, address = [1,2]},
#     { name = "frequency",    byte_order = "AB",   data_type = "UFIXED", scale=0.1,  address = [7]},
#     { name = "power",        byte_order = "ABCD", data_type = "UFIXED", scale=0.1,  address = [3,4]},
#   ]
#   input_registers = [
#     { name = "tank_level",   byte_order = "AB",   data_type = "INT16",   scale=1.0,     address = [0]},
#     { name = "tank_ph",      byte_order = "AB",   data_type = "INT16",   scale=1.0,     address = [1]},
#     { name = "pump1_speed",  byte_order = "ABCD", data_type = "INT32",   scale=1.0,     address = [3,4]},
#   ]
#
#   ## --- "request" configuration style ---
#
#   ## Per request definition
#   ##
#
#   ## Define a request sent to the device
#   ## Multiple of those requests can be defined. Data will be collated into metrics at the end of data collection.
#   [[inputs.modbus.request]]
#     ## ID of the modbus slave device to query.
#     ## If you need to query multiple slave-devices, create several "request" definitions.
#     slave_id = 1
#
#     ## Byte order of the data.
#     ##  |---ABCD -- Big Endian (Motorola)
#     ##  |---DCBA -- Little Endian (Intel)
#     ##  |---BADC -- Big Endian with byte swap
#     ##  |---CDAB -- Little Endian with byte swap
#     byte_order = "ABCD"
#
#     ## Type of the register for the request
#     ## Can be "coil", "discrete", "holding" or "input"
#     register = "coil"
#
#     ## Name of the measurement.
#     ## Can be overriden by the individual field definitions. Defaults to "modbus"
#     # measurement = "modbus"
#
#     ## Request optimization algorithm.
#     ##  |---none       -- Do not perform any optimization and use the given layout(default)
#     ##  |---shrink     -- Shrink requests to actually requested fields
#     ##  |                 by stripping leading and trailing omits
#     ##  |---rearrange  -- Rearrange request boundaries within consecutive address ranges
#     ##  |                 to reduce the number of requested registers by keeping
#     ##  |                 the number of requests.
#     ##  |---max_insert -- Rearrange request keeping the number of extra fields below the value
#     ##                    provided in "optimization_max_register_fill". It is not necessary to define 'omitted'
#     ##                    fields as the optimisation will add such field only where needed.
#     # optimization = "none"
#
#     ## Maximum number register the optimizer is allowed to insert between two fields to
#     ## save requests.
#     ## This option is only used for the 'max_insert' optimization strategy.
#     ## NOTE: All omitted fields are ignored, so this option denotes the effective hole
#     ## size to fill.
#     # optimization_max_register_fill = 50
#
#     ## Field definitions
#     ## Analog Variables, Input Registers and Holding Registers
#     ## address        - address of the register to query. For coil and discrete inputs this is the bit address.
#     ## name *1        - field name
#     ## type *1,2      - type of the modbus field, can be
#     ##                  INT8L, INT8H, UINT8L, UINT8H (low and high byte variants)
#     ##                  INT16, UINT16, INT32, UINT32, INT64, UINT64 and
#     ##                  FLOAT16, FLOAT32, FLOAT64 (IEEE 754 binary representation)
#     ## scale *1,2     - (optional) factor to scale the variable with
#     ## output *1,3    - (optional) type of resulting field, can be INT64, UINT64 or FLOAT64. Defaults to FLOAT64 if
#     ##                  "scale" is provided and to the input "type" class otherwise (i.e. INT* -> INT64, etc).
#     ## measurement *1 - (optional) measurement name, defaults to the setting of the request
#     ## omit           - (optional) omit this field. Useful to leave out single values when querying many registers
#     ##                  with a single request. Defaults to "false".
#     ##
#     ## *1: These fields are ignored if field is omitted ("omit"=true)
#     ## *2: These fields are ignored for both "coil" and "discrete"-input type of registers.
#     ## *3: This field can only be "UINT16" or "BOOL" if specified for both "coil"
#     ##     and "discrete"-input type of registers. By default the fields are
#     ##     output as zero or one in UINT16 format unless "BOOL" is used.
#
#     ## Coil / discrete input example
#     fields = [
#       { address=0, name="motor1_run"},
#       { address=1, name="jog", measurement="motor"},
#       { address=2, name="motor1_stop", omit=true},
#       { address=3, name="motor1_overheating", output="BOOL"},
#     ]
#
#     [inputs.modbus.request.tags]
#       machine = "impresser"
#       location = "main building"
#
#   [[inputs.modbus.request]]
#     ## Holding example
#     ## All of those examples will result in FLOAT64 field outputs
#     slave_id = 1
#     byte_order = "DCBA"
#     register = "holding"
#     fields = [
#       { address=0, name="voltage",      type="INT16",   scale=0.1   },
#       { address=1, name="current",      type="INT32",   scale=0.001 },
#       { address=3, name="power",        type="UINT32",  omit=true   },
#       { address=5, name="energy",       type="FLOAT32", scale=0.001, measurement="W" },
#       { address=7, name="frequency",    type="UINT32",  scale=0.1   },
#       { address=8, name="power_factor", type="INT64",   scale=0.01  },
#     ]
#
#     [inputs.modbus.request.tags]
#       machine = "impresser"
#       location = "main building"
#
#   [[inputs.modbus.request]]
#     ## Input example with type conversions
#     slave_id = 1
#     byte_order = "ABCD"
#     register = "input"
#     fields = [
#       { address=0, name="rpm",         type="INT16"                   },  # will result in INT64 field
#       { address=1, name="temperature", type="INT16", scale=0.1        },  # will result in FLOAT64 field
#       { address=2, name="force",       type="INT32", output="FLOAT64" },  # will result in FLOAT64 field
#       { address=4, name="hours",       type="UINT32"                  },  # will result in UIN64 field
#     ]
#
#     [inputs.modbus.request.tags]
#       machine = "impresser"
#       location = "main building"
#
#   ## --- "metric" configuration style ---
#
#   ## Per metric definition
#   ##
#
#   ## Request optimization algorithm across metrics
#   ##  |---none       -- Do not perform any optimization and just group requests
#   ##  |                 within metrics (default)
#   ##  |---max_insert -- Collate registers across all defined metrics and fill in
#   ##                    holes to optimize the number of requests.
#   # optimization = "none"
#
#   ## Maximum number of registers the optimizer is allowed to insert between
#   ## non-consecutive registers to save requests.
#   ## This option is only used for the 'max_insert' optimization strategy and
#   ## effectively denotes the hole size between registers to fill.
#   # optimization_max_register_fill = 50
#
#   ## Define a metric produced by the requests to the device
#   ## Multiple of those metrics can be defined. The referenced registers will
#   ## be collated into requests send to the device
#   [[inputs.modbus.metric]]
#     ## ID of the modbus slave device to query
#     ## If you need to query multiple slave-devices, create several "metric" definitions.
#     slave_id = 1
#
#     ## Byte order of the data
#     ##  |---ABCD -- Big Endian (Motorola)
#     ##  |---DCBA -- Little Endian (Intel)
#     ##  |---BADC -- Big Endian with byte swap
#     ##  |---CDAB -- Little Endian with byte swap
#     # byte_order = "ABCD"
#
#     ## Name of the measurement
#     # measurement = "modbus"
#
#     ## Field definitions
#     ## register   - type of the modbus register, can be "coil", "discrete",
#     ##              "holding" or "input". Defaults to "holding".
#     ## address    - address of the register to query. For coil and discrete inputs this is the bit address.
#     ## name       - field name
#     ## type *1    - type of the modbus field, can be
#     ##                  INT8L, INT8H, UINT8L, UINT8H (low and high byte variants)
#     ##                  INT16, UINT16, INT32, UINT32, INT64, UINT64 and
#     ##                  FLOAT16, FLOAT32, FLOAT64 (IEEE 754 binary representation)
#     ## scale *1   - (optional) factor to scale the variable with
#     ## output *2  - (optional) type of resulting field, can be INT64, UINT64 or FLOAT64. Defaults to FLOAT64 if
#     ##              "scale" is provided and to the input "type" class otherwise (i.e. INT* -> INT64, etc).
#     ##
#     ## *1: These fields are ignored for both "coil" and "discrete"-input type of registers.
#     ## *2: This field can only be "UINT16" or "BOOL" if specified for both "coil"
#     ##     and "discrete"-input type of registers. By default the fields are
#     ##     output as zero or one in UINT16 format unless "BOOL" is used.
#     fields = [
#       { register="coil",    address=0, name="door_open"},
#       { register="coil",    address=1, name="status_ok"},
#       { register="holding", address=0, name="voltage",      type="INT16"   },
#       { address=1, name="current",      type="INT32",   scale=0.001 },
#       { address=5, name="energy",       type="FLOAT32", scale=0.001,},
#       { address=7, name="frequency",    type="UINT32",  scale=0.1   },
#       { address=8, name="power_factor", type="INT64",   scale=0.01  },
#     ]
#
#     ## Tags assigned to the metric
#     # [inputs.modbus.metric.tags]
#     #   machine = "impresser"
#     #   location = "main building"
#
#
#   ## RS485 specific settings. Only take effect for serial controllers.
#   ## Note: This has to be at the end of the modbus configuration due to
#   ## TOML constraints.
#   # [inputs.modbus.rs485]
#     ## Delay RTS prior to sending
#     # delay_rts_before_send = "0ms"
#     ## Delay RTS after to sending
#     # delay_rts_after_send = "0ms"
#     ## Pull RTS line to high during sending
#     # rts_high_during_send = false
#     ## Pull RTS line to high after sending
#     # rts_high_after_send = false
#     ## Enabling receiving (Rx) during transmission (Tx)
#     # rx_during_tx = false
#
#   ## Enable workarounds required by some devices to work correctly
#   # [inputs.modbus.workarounds]
#     ## Pause after connect delays the first request by the specified time.
#     ## This might be necessary for (slow) devices.
#     # pause_after_connect = "0ms"
#
#     ## Pause between read requests sent to the device.
#     ## This might be necessary for (slow) serial devices.
#     # pause_between_requests = "0ms"
#
#     ## Close the connection after every gather cycle.
#     ## Usually the plugin closes the connection after a certain idle-timeout,
#     ## however, if you query a device with limited simultaneous connectivity
#     ## (e.g. serial devices) from multiple instances you might want to only
#     ## stay connected during gather and disconnect afterwards.
#     # close_connection_after_gather = false
#
#     ## Force the plugin to read each field in a separate request.
#     ## This might be necessary for devices not conforming to the spec,
#     ## see https://github.com/influxdata/telegraf/issues/12071.
#     # one_request_per_field = false
#
#     ## Enforce the starting address to be zero for the first request on
#     ## coil registers. This is necessary for some devices see
#     ## https://github.com/influxdata/telegraf/issues/8905


# # Read Nginx's basic status information (ngx_http_stub_status_module)
# [[inputs.nginx]]
#   ## An array of Nginx stub_status URI to gather stats.
#   urls = ["http://localhost/server_status"]
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## HTTP response timeout (default: 5s)
#   response_timeout = "5s"


# # Read metrics of phpfpm, via HTTP status page or socket
# [[inputs.phpfpm]]
#   ## An array of addresses to gather stats about. Specify an ip or hostname
#   ## with optional port and path
#   ##
#   ## Plugin can be configured in three modes (either can be used):
#   ##   - http: the URL must start with http:// or https://, ie:
#   ##       "http://localhost/status"
#   ##       "http://192.168.130.1/status?full"
#   ##
#   ##   - unixsocket: path to fpm socket, ie:
#   ##       "/var/run/php5-fpm.sock"
#   ##      or using a custom fpm status path:
#   ##       "/var/run/php5-fpm.sock:fpm-custom-status-path"
#   ##      glob patterns are also supported:
#   ##       "/var/run/php*.sock"
#   ##
#   ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:
#   ##       "fcgi://10.0.0.12:9000/status"
#   ##       "cgi://10.0.10.12:9001/status"
#   ##
#   ## Example of multiple gathering from local socket and remote host
#   ## urls = ["http://192.168.1.20/status", "/tmp/fpm.sock"]
#   urls = ["http://localhost/status"]
#
#   ## Duration allowed to complete HTTP requests.
#   # timeout = "5s"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false


# # Ping given url(s) and return statistics
# [[inputs.ping]]
#   ## Hosts to send ping packets to.
#   urls = ["example.org"]
#
#   ## Method used for sending pings, can be either "exec" or "native".  When set
#   ## to "exec" the systems ping command will be executed.  When set to "native"
#   ## the plugin will send pings directly.
#   ##
#   ## While the default is "exec" for backwards compatibility, new deployments
#   ## are encouraged to use the "native" method for improved compatibility and
#   ## performance.
#   # method = "exec"
#
#   ## Number of ping packets to send per interval.  Corresponds to the "-c"
#   ## option of the ping command.
#   # count = 1
#
#   ## Time to wait between sending ping packets in seconds.  Operates like the
#   ## "-i" option of the ping command.
#   # ping_interval = 1.0
#
#   ## If set, the time to wait for a ping response in seconds.  Operates like
#   ## the "-W" option of the ping command.
#   # timeout = 1.0
#
#   ## If set, the total ping deadline, in seconds.  Operates like the -w option
#   ## of the ping command.
#   # deadline = 10
#
#   ## Interface or source address to send ping from.  Operates like the -I or -S
#   ## option of the ping command.
#   # interface = ""
#
#   ## Percentiles to calculate. This only works with the native method.
#   # percentiles = [50, 95, 99]
#
#   ## Specify the ping executable binary.
#   # binary = "ping"
#
#   ## Arguments for ping command. When arguments is not empty, the command from
#   ## the binary option will be used and other options (ping_interval, timeout,
#   ## etc) will be ignored.
#   # arguments = ["-c", "3"]
#
#   ## Use only IPv6 addresses when resolving a hostname.
#   # ipv6 = false
#
#   ## Number of data bytes to be sent. Corresponds to the "-s"
#   ## option of the ping command. This only works with the native method.
#   # size = 56


# # Reads metrics from RabbitMQ servers via the Management Plugin
# [[inputs.rabbitmq]]
#   ## Management Plugin url. (default: http://localhost:15672)
#   # url = "http://localhost:15672"
#   ## Tag added to rabbitmq_overview series; deprecated: use tags
#   # name = "rmq-server-1"
#   ## Credentials
#   # username = "guest"
#   # password = "guest"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Optional request timeouts
#   ##
#   ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait
#   ## for a server's response headers after fully writing the request.
#   # header_timeout = "3s"
#   ##
#   ## client_timeout specifies a time limit for requests made by this client.
#   ## Includes connection time, any redirects, and reading the response body.
#   # client_timeout = "4s"
#
#   ## A list of nodes to gather as the rabbitmq_node measurement. If not
#   ## specified, metrics for all nodes are gathered.
#   # nodes = ["rabbit@node1", "rabbit@node2"]
#
#   ## A list of queues to gather as the rabbitmq_queue measurement. If not
#   ## specified, metrics for all queues are gathered.
#   ## Deprecated in 1.6: Use queue_name_include instead.
#   # queues = ["telegraf"]
#
#   ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not
#   ## specified, metrics for all exchanges are gathered.
#   # exchanges = ["telegraf"]
#
#   ## Metrics to include and exclude. Globs accepted.
#   ## Note that an empty array for both will include all metrics
#   ## Currently the following metrics are supported: "exchange", "federation", "node", "overview", "queue"
#   # metric_include = []
#   # metric_exclude = []
#
#   ## Queues to include and exclude. Globs accepted.
#   ## Note that an empty array for both will include all queues
#   # queue_name_include = []
#   # queue_name_exclude = []
#
#   ## Federation upstreams to include and exclude specified as an array of glob
#   ## pattern strings.  Federation links can also be limited by the queue and
#   ## exchange filters.
#   # federation_upstream_include = []
#   # federation_upstream_exclude = []


###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################


# # Read metrics from one or many ClickHouse servers
# [[inputs.clickhouse]]
#   ## Username for authorization on ClickHouse server
#   username = "default"
#
#   ## Password for authorization on ClickHouse server
#   # password = ""
#
#   ## HTTP(s) timeout while getting metrics values
#   ## The timeout includes connection time, any redirects, and reading the
#   ## response body.
#   # timeout = 5s
#
#   ## List of servers for metrics scraping
#   ## metrics scrape via HTTP(s) clickhouse interface
#   ## https://clickhouse.tech/docs/en/interfaces/http/
#   servers = ["http://127.0.0.1:8123"]
#
#   ## If "auto_discovery"" is "true" plugin tries to connect to all servers
#   ## available in the cluster with using same "user:password" described in
#   ## "user" and "password" parameters and get this server hostname list from
#   ## "system.clusters" table. See
#   ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters
#   ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers
#   ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/
#   ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables
#   # auto_discovery = true
#
#   ## Filter cluster names in "system.clusters" when "auto_discovery" is "true"
#   ## when this filter present then "WHERE cluster IN (...)" filter will apply
#   ## please use only full cluster names here, regexp and glob filters is not
#   ## allowed for "/etc/clickhouse-server/config.d/remote.xml"
#   ## <yandex>
#   ##  <remote_servers>
#   ##    <my-own-cluster>
#   ##        <shard>
#   ##          <replica><host>clickhouse-ru-1.local</host><port>9000</port></replica>
#   ##          <replica><host>clickhouse-ru-2.local</host><port>9000</port></replica>
#   ##        </shard>
#   ##        <shard>
#   ##          <replica><host>clickhouse-eu-1.local</host><port>9000</port></replica>
#   ##          <replica><host>clickhouse-eu-2.local</host><port>9000</port></replica>
#   ##        </shard>
#   ##    </my-onw-cluster>
#   ##  </remote_servers>
#   ##
#   ## </yandex>
#   ##
#   ## example: cluster_include = ["my-own-cluster"]
#   # cluster_include = []
#
#   ## Filter cluster names in "system.clusters" when "auto_discovery" is
#   ## "true" when this filter present then "WHERE cluster NOT IN (...)"
#   ## filter will apply
#   ##    example: cluster_exclude = ["my-internal-not-discovered-cluster"]
#   # cluster_exclude = []
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false

#  ## DEPRECATED: The "logparser" plugin is deprecated in version 1.15.0, use 'inputs.tail' with 'grok' data format instead.
# # Read metrics off Arista LANZ, via socket
# [[inputs.logparser]]
#   ## Log files to parse.
#   ## These accept standard unix glob matching rules, but with the addition of
#   ## ** as a "super asterisk". ie:
#   ##   /var/log/**.log     -> recursively find all .log files in /var/log
#   ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log
#   ##   /var/log/apache.log -> only tail the apache log file
#   files = ["/var/log/apache/access.log"]
#
#   ## Read files that currently exist from the beginning. Files that are created
#   ## while telegraf is running (and that match the "files" globs) will always
#   ## be read from the beginning.
#   from_beginning = false
#
#   ## Method used to watch for file updates.  Can be either "inotify" or "poll".
#   # watch_method = "inotify"
#
#   ## Parse logstash-style "grok" patterns:
#   [inputs.logparser.grok]
#     ## This is a list of patterns to check the given log file(s) for.
#     ## Note that adding patterns here increases processing time. The most
#     ## efficient configuration is to have one pattern per logparser.
#     ## Other common built-in patterns are:
#     ##   %{COMMON_LOG_FORMAT}   (plain apache & nginx access logs)
#     ##   %{COMBINED_LOG_FORMAT} (access logs + referrer & agent)
#     patterns = ["%{COMBINED_LOG_FORMAT}"]
#
#     ## Name of the outputted measurement name.
#     measurement = "apache_access_log"
#
#     ## Full path(s) to custom pattern files.
#     custom_pattern_files = []
#
#     ## Custom patterns can also be defined here. Put one pattern per line.
#     custom_patterns = '''
#     '''
#
#     ## Timezone allows you to provide an override for timestamps that
#     ## don't already include an offset
#     ## e.g. 04/06/2016 12:41:45 data one two 5.43Âµs
#     ##
#     ## Default: "" which renders UTC
#     ## Options are as follows:
#     ##   1. Local             -- interpret based on machine localtime
#     ##   2. "Canada/Eastern"  -- Unix TZ values like those found in https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
#     ##   3. UTC               -- or blank/unspecified, will return timestamp in UTC
#     # timezone = "Canada/Eastern"
#
#     ## When set to "disable", timestamp will not incremented if there is a
#     ## duplicate.
#     # unique_timestamp = "auto"


# # Read metrics from MQTT topic(s)
# [[inputs.mqtt_consumer]]
#   ## Broker URLs for the MQTT server or cluster.  To connect to multiple
#   ## clusters or standalone servers, use a separate plugin instance.
#   ##   example: servers = ["tcp://localhost:1883"]
#   ##            servers = ["ssl://localhost:1883"]
#   ##            servers = ["ws://localhost:1883"]
#   servers = ["tcp://127.0.0.1:1883"]
#
#   ## Topics that will be subscribed to.
#   topics = [
#     "telegraf/host01/cpu",
#     "telegraf/+/mem",
#     "sensors/#",
#   ]
#
#   ## The message topic will be stored in a tag specified by this value.  If set
#   ## to the empty string no topic tag will be created.
#   # topic_tag = "topic"
#
#   ## QoS policy for messages
#   ##   0 = at most once
#   ##   1 = at least once
#   ##   2 = exactly once
#   ##
#   ## When using a QoS of 1 or 2, you should enable persistent_session to allow
#   ## resuming unacknowledged messages.
#   # qos = 0
#
#   ## Connection timeout for initial connection in seconds
#   # connection_timeout = "30s"
#
#   ## Max undelivered messages
#   ## This plugin uses tracking metrics, which ensure messages are read to
#   ## outputs before acknowledging them to the original broker to ensure data
#   ## is not lost. This option sets the maximum messages to read from the
#   ## broker that have not been written by an output.
#   ##
#   ## This value needs to be picked with awareness of the agent's
#   ## metric_batch_size value as well. Setting max undelivered messages too high
#   ## can result in a constant stream of data batches to the output. While
#   ## setting it too low may never flush the broker's messages.
#   # max_undelivered_messages = 1000
#
#   ## Persistent session disables clearing of the client session on connection.
#   ## In order for this option to work you must also set client_id to identify
#   ## the client.  To receive messages that arrived while the client is offline,
#   ## also set the qos option to 1 or 2 and don't forget to also set the QoS when
#   ## publishing.
#   # persistent_session = false
#
#   ## If unset, a random client ID will be generated.
#   # client_id = ""
#
#   ## Username and password to connect MQTT server.
#   # username = "telegraf"
#   # password = "metricsmetricsmetricsmetrics"
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Client trace messages
#   ## When set to true, and debug mode enabled in the agent settings, the MQTT
#   ## client's messages are included in telegraf logs. These messages are very
#   ## noisey, but essential for debugging issues.
#   # client_trace = false
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ## Enable extracting tag values from MQTT topics
#   ## _ denotes an ignored entry in the topic path
#   # [[inputs.mqtt_consumer.topic_parsing]]
#   #   topic = ""
#   #   measurement = ""
#   #   tags = ""
#   #   fields = ""
#   ## Value supported is int, float, unit
#   #   [[inputs.mqtt_consumer.topic.types]]
#   #      key = type

# # Read metrics from one or many prometheus clients
# [[inputs.prometheus]]
#   ## An array of urls to scrape metrics from.
#   urls = ["http://localhost:9100/metrics"]
#
#   ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.
#   ## See "Metric Format Configuration" in plugins/inputs/prometheus/README.md for details.
#   ## Valid options: 1, 2
#   # metric_version = 1
#
#   ## Url tag name (tag containing scrapped url. optional, default is "url")
#   # url_tag = "url"
#
#   ## Whether the timestamp of the scraped metrics will be ignored.
#   ## If set to true, the gather time will be used.
#   # ignore_timestamp = false
#
#   ## An array of Kubernetes services to scrape metrics from.
#   # kubernetes_services = ["http://my-service-dns.my-namespace:9100/metrics"]
#
#   ## Kubernetes config file to create client from.
#   # kube_config = "/path/to/kubernetes.config"
#
#   ## Scrape Pods
#   ## Enable scraping of k8s pods. Further settings as to which pods to scape
#   ## are determiend by the 'method' option below. When enabled, the default is
#   ## to use annotations to determine whether to scrape or not.
#   # monitor_kubernetes_pods = false
#
#   ## Scrape Pods Method
#   ## annotations: default, looks for specific pod annotations documented below
#   ## settings: only look for pods matching the settings provided, not
#   ##   annotations
#   ## settings+annotations: looks at pods that match annotations using the user
#   ##   defined settings
#   # monitor_kubernetes_pods_method = "annotations"
#
#   ## Scrape Pods 'annotations' method options
#   ## If set method is set to 'annotations' or 'settings+annotations', these
#   ## annotation flags are looked for:
#   ## - prometheus.io/scrape: Required to enable scraping for this pod. Can also
#   ##     use 'prometheus.io/scrape=false' annotation to opt-out entirely.
#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will
#   ##     need to set this to 'https' & most likely set the tls config
#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with
#   ##     this annotation
#   ## - prometheus.io/port: If port is not 9102 use this annotation
#
#   ## Scrape Pods 'settings' method options
#   ## When using 'settings' or 'settings+annotations', the default values for
#   ## annotations can be modified using with the following options:
#   # monitor_kubernetes_pods_scheme = "http"
#   # monitor_kubernetes_pods_port = "9102"
#   # monitor_kubernetes_pods_path = "/metrics"
#
#   ## Get the list of pods to scrape with either the scope of
#   ## - cluster: the kubernetes watch api (default, no need to specify)
#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.
#   # pod_scrape_scope = "cluster"
#
#   ## Only for node scrape scope: node IP of the node that telegraf is running on.
#   ## Either this config or the environment variable NODE_IP must be set.
#   # node_ip = "10.180.1.1"
#
#   ## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.
#   ## Default is 60 seconds.
#   # pod_scrape_interval = 60
#
#   ## Restricts Kubernetes monitoring to a single namespace
#   ##   ex: monitor_kubernetes_pods_namespace = "default"
#   # monitor_kubernetes_pods_namespace = ""
#   ## The name of the label for the pod that is being scraped.
#   ## Default is 'namespace' but this can conflict with metrics that have the label 'namespace'
#   # pod_namespace_label_name = "namespace"
#   # label selector to target pods which have the label
#   # kubernetes_label_selector = "env=dev,app=nginx"
#   # field selector to target pods
#   # eg. To scrape pods on a specific node
#   # kubernetes_field_selector = "spec.nodeName=$HOSTNAME"
#
#   ## Filter which pod annotations and labels will be added to metric tags
#   #
#   # pod_annotation_include = ["annotation-key-1"]
#   # pod_annotation_exclude = ["exclude-me"]
#   # pod_label_include = ["label-key-1"]
#   # pod_label_exclude = ["exclude-me"]
#
#   # cache refresh interval to set the interval for re-sync of pods list.
#   # Default is 60 minutes.
#   # cache_refresh_interval = 60
#
#   ## Scrape Services available in Consul Catalog
#   # [inputs.prometheus.consul]
#   #   enabled = true
#   #   agent = "http://localhost:8500"
#   #   query_interval = "5m"
#
#   #   [[inputs.prometheus.consul.query]]
#   #     name = "a service name"
#   #     tag = "a service tag"
#   #     url = 'http://{{if ne .ServiceAddress ""}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'
#   #     [inputs.prometheus.consul.query.tags]
#   #       host = "{{.Node}}"
#
#   ## Use bearer token for authorization. ('bearer_token' takes priority)
#   # bearer_token = "/path/to/bearer/token"
#   ## OR
#   # bearer_token_string = "abc_123"
#
#   ## HTTP Basic Authentication username and password. ('bearer_token' and
#   ## 'bearer_token_string' take priority)
#   # username = ""
#   # password = ""
#
#   ## Optional custom HTTP headers
#   # http_headers = {"X-Special-Header" = "Special-Value"}
#
#   ## Specify timeout duration for slower prometheus clients (default is 5s)
#   # timeout = "5s"
#
#   ## deprecated in 1.26; use the timeout option
#   # response_timeout = "5s"
#
#   ## HTTP Proxy support
#   # use_system_proxy = false
#   # http_proxy_url = ""
#
#   ## Optional TLS Config
#   # tls_ca = /path/to/cafile
#   # tls_cert = /path/to/certfile
#   # tls_key = /path/to/keyfile
#
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = false
#
#   ## Use the given name as the SNI server name on each URL
#   # tls_server_name = "myhost.example.org"
#
#   ## TLS renegotiation method, choose from "never", "once", "freely"
#   # tls_renegotiation_method = "never"
#
#   ## Enable/disable TLS
#   ## Set to true/false to enforce TLS being enabled/disabled. If not set,
#   ## enable TLS only if any of the other options are specified.
#   # tls_enable = true
#
#   ## Control pod scraping based on pod namespace annotations
#   ## Pass and drop here act like tagpass and tagdrop, but instead
#   ## of filtering metrics they filters pod candidates for scraping
#   #[inputs.prometheus.namespace_annotation_pass]
#   # annotation_key = ["value1", "value2"]
#   #[inputs.prometheus.namespace_annotation_drop]
#   # some_annotation_key = ["dont-scrape"]

# # Read metrics from one or many redis servers
# [[inputs.redis]]
#   ## specify servers via a url matching:
#   ##  [protocol://][username:password]@address[:port]
#   ##  e.g.
#   ##    tcp://localhost:6379
#   ##    tcp://username:password@192.168.99.100
#   ##    unix:///var/run/redis.sock
#   ##
#   ## If no servers are specified, then localhost is used as the host.
#   ## If no port is specified, 6379 is used
#   servers = ["tcp://localhost:6379"]
#
#   ## Optional. Specify redis commands to retrieve values
#   # [[inputs.redis.commands]]
#   #   # The command to run where each argument is a separate element
#   #   command = ["get", "sample-key"]
#   #   # The field to store the result in
#   #   field = "sample-key-value"
#   #   # The type of the result
#   #   # Can be "string", "integer", or "float"
#   #   type = "string"
#
#   ## Specify username and password for ACL auth (Redis 6.0+). You can add this
#   ## to the server URI above or specify it here. The values here take
#   ## precidence.
#   # username = ""
#   # password = ""
#
#   ## Optional TLS Config
#   # tls_ca = "/etc/telegraf/ca.pem"
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#   ## Use TLS but skip chain & host verification
#   # insecure_skip_verify = true
